#.github/workflows/cicd.yml
name: Deploy to AWS EKS

on:
  push:
    branches:
      - eks-check  

jobs:
  deploy:
    runs-on: ubuntu-latest

    env:
      TF_WORKING_DIR: ./terraform-related/AWS_terraform_grafana
      ARGOCD_PASSWORD_HASH: $2a$10$8K9jKpK7a9c6xL8zFiZ65uHpYz9ROzAyl3PzGp6u2Fj0CeLccL5tK

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v2

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ vars.AWS_REGION || 'ap-northeast-2' }}

      - name: Set Secrets as Environment Variables
        run: |
          echo "TF_VAR_gemini_api_key=${{ secrets.GEMINI_API_KEY }}" >> $GITHUB_ENV
          echo "TF_VAR_azure_api_key=${{ secrets.AZURE_API_KEY }}" >> $GITHUB_ENV
          echo "TF_VAR_azure_api_base=${{ secrets.AZURE_API_BASE }}" >> $GITHUB_ENV
          echo "TF_VAR_azure_api_version=${{ secrets.AZURE_API_VERSION }}" >> $GITHUB_ENV
          echo "TF_VAR_postgres_db=${{ secrets.POSTGRES_DB }}" >> $GITHUB_ENV
          echo "TF_VAR_postgres_user=${{ secrets.POSTGRES_USER }}" >> $GITHUB_ENV
          echo "TF_VAR_postgres_password=${{ secrets.POSTGRES_PASSWORD }}" >> $GITHUB_ENV
          echo "TF_VAR_database_url=${{ secrets.DATABASE_URL }}" >> $GITHUB_ENV
          echo "TF_VAR_litellm_master_key=${{ secrets.LITELLM_MASTER_KEY }}" >> $GITHUB_ENV
          echo "TF_VAR_litellm_salt_key=${{ secrets.LITELLM_SALT_KEY }}" >> $GITHUB_ENV

      - name: Terraform Init
        run: terraform init
        working-directory: ${{ env.TF_WORKING_DIR }}

      - name: Terraform Plan
        run: terraform plan -var-file="terraform.tfvars" -input=false
        working-directory: ${{ env.TF_WORKING_DIR }}

      - name: Terraform Apply
        run: terraform apply -auto-approve -var-file="terraform.tfvars" -input=false
        working-directory: ${{ env.TF_WORKING_DIR }}

      - name: Update Kubeconfig
        run: aws eks update-kubeconfig --region ap-northeast-2 --name openwebui

      - name: Apply aws-auth ConfigMap and wait for node
        run: |
          echo "Getting nodegroup role ARN..."
          NODE_ROLE_ARN=$(aws eks describe-nodegroup --cluster-name openwebui --nodegroup-name openwebui-nodegroup --query 'nodegroup.nodeRole' --output text)
  
          echo "Generating aws-auth ConfigMap with correct ARN: $NODE_ROLE_ARN"
          cat > aws-auth.yaml << EOF
          apiVersion: v1
          kind: ConfigMap
          metadata:
            name: aws-auth
            namespace: kube-system
          data:
            mapRoles: |
              - rolearn: $NODE_ROLE_ARN
                username: system:node:{{EC2PrivateDNSName}}
                groups:
                  - system:bootstrappers
                  - system:nodes
          EOF
  
          echo "Applying generated aws-auth ConfigMap..."
          kubectl apply -f aws-auth.yaml -n kube-system
  
          echo "Waiting for EKS node to register (up to 5 minutes)..."
          for i in {1..30}; do
            COUNT=$(kubectl get nodes --no-headers 2>/dev/null | wc -l)
            if [ "$COUNT" -gt 0 ]; then
              echo "✅ EKS node registered!"
              break
            fi
            echo "⏳ Still waiting for node... ($i/30)"
            sleep 10
          done
  
          if [ "$COUNT" -eq 0 ]; then
            echo "❌ Timeout: No EKS nodes registered after 5 minutes."
            kubectl get pods -n kube-system
            kubectl describe configmap aws-auth -n kube-system
            exit 1
          fi

      # ===== 필수 컴포넌트 설치 추가 =====
      - name: Install Helm
        uses: azure/setup-helm@v3
        with:
          version: 'latest'

      - name: Install AWS Load Balancer Controller
        run: |
          # IAM Policy 생성
          curl -O https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/v2.7.0/docs/install/iam_policy.json
          
          aws iam create-policy \
            --policy-name AWSLoadBalancerControllerIAMPolicy-${{ github.run_id }} \
            --policy-document file://iam_policy.json || true
          
          # CRDs 설치
          kubectl apply -k "https://github.com/aws/eks-charts/stable/aws-load-balancer-controller/crds?ref=master"
          
          # Helm repo 추가
          helm repo add eks https://aws.github.io/eks-charts
          helm repo update
          
          # AWS Load Balancer Controller 설치
          helm install aws-load-balancer-controller eks/aws-load-balancer-controller \
            -n kube-system \
            --set clusterName=openwebui \
            --set serviceAccount.create=true

      - name: Install Metrics Server
        run: |
          kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml

      - name: Install EBS CSI Driver
        run: |
          # EBS CSI Driver addon 추가
          aws eks create-addon \
            --cluster-name openwebui \
            --addon-name aws-ebs-csi-driver \
            --resolve-conflicts OVERWRITE || true
          
          # Addon이 준비될 때까지 대기
          sleep 60

      - name: Create StorageClass
        run: |
          cat <<EOF | kubectl apply -f -
          apiVersion: storage.k8s.io/v1
          kind: StorageClass
          metadata:
            name: gp3
          provisioner: ebs.csi.aws.com
          parameters:
            type: gp3
            fsType: ext4
          volumeBindingMode: WaitForFirstConsumer
          allowVolumeExpansion: true
          EOF
      # ===== 여기까지 추가 =====

      - name: Install ArgoCD
        run: |
          kubectl create namespace argocd --dry-run=client -o yaml | kubectl apply -f -
          kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml

      - name: Wait for ArgoCD to be Ready
        run: |
          echo "Waiting for ArgoCD server pod to be ready..."
          kubectl wait --for=condition=available --timeout=180s deployment/argocd-server -n argocd

      - name: Expose ArgoCD Server with LoadBalancer
        run: kubectl apply -f argocd/argocd-server-service.yaml

      - name: Apply ArgoCD Application
        run: kubectl apply -f argocd/argocd-app.yaml

      # ArgoCD가 아닌 직접 배포로 변경 (ArgoCD와 충돌 방지)
      - name: Apply Kubernetes Manifests
        run: |
          # 디렉토리가 존재하는지 확인하고 적용
          if [ -d "kubernetes/manifests/storage" ]; then
            kubectl apply -f kubernetes/manifests/storage/
          fi
          
          if [ -d "kubernetes/manifests/database" ]; then
            kubectl apply -f kubernetes/manifests/database/
          fi
          
          # 기본 manifests 적용
          kubectl apply -f kubernetes/manifests/
          
          if [ -d "kubernetes/manifests/autoscaling" ]; then
            kubectl apply -f kubernetes/manifests/autoscaling/
          fi
          
          if [ -d "kubernetes/manifests/network" ]; then
            kubectl apply -f kubernetes/manifests/network/
          fi
          
          if [ -d "kubernetes/manifests/ingress" ]; then
            kubectl apply -f kubernetes/manifests/ingress/
          fi
          
          if [ -d "kubernetes/manifests/monitoring" ]; then
            kubectl apply -f kubernetes/manifests/monitoring/
          fi

      - name: Wait for Deployments
        run: |
            for DEPLOY in openwebui-v1 openwebui-v2 litellm nginx-ab-proxy; do
              if kubectl get deployment "$DEPLOY" -n default &> /dev/null; then
                echo "Waiting for deployment $DEPLOY to complete..."
                kubectl rollout status deployment/"$DEPLOY" -n default --timeout=300s
              else
                echo "Deployment $DEPLOY not found. Skipping."
              fi
            done
        

      - name: Health Check
        run: |
          echo "Waiting for all pods to be ready..."
          kubectl wait --for=condition=ready pod -l app=openwebui-v1 --timeout=300s || true
          kubectl wait --for=condition=ready pod -l app=litellm --timeout=300s || true
          kubectl wait --for=condition=ready pod -l app=nginx-ab-proxy --timeout=300s || true
       
      - name: Rollback on Failure
        if: failure()
        run: |
          echo "Deployment failed. Rolling back..."
          kubectl rollout undo deployment/openwebui-v1 || true
          kubectl rollout undo deployment/openwebui-v2 || true
          kubectl rollout undo deployment/litellm || true
          kubectl rollout undo deployment/nginx-ab-proxy || true
          exit 1

      - name: Get Load Balancer URL
        run: |
          echo "Waiting for Load Balancer to be ready..."
          sleep 30
          
          # ArgoCD LoadBalancer URL
          ARGOCD_LB=$(kubectl get svc argocd-server -n argocd -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' || echo "Not ready")
          echo "ArgoCD available at: http://$ARGOCD_LB"
          
          # Application LoadBalancer URL (Ingress가 있는 경우)
          if kubectl get ingress openwebui-ingress -n default 2>/dev/null; then
            LB_URL=$(kubectl get ingress openwebui-ingress -n default -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' || echo "Not ready")
            echo "Application available at: http://$LB_URL"
          fi